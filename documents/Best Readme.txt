WWI â€” Google Location History Analyzer
This stands for "Where Was I"

Analyze and visualize your Google Location History with local privacy and detailed travel insights.

Why this project?

Googleâ€™s Location History exports are massive and hard to use. This app normalizes those JSON files, enriches them with geocoding, and generates clear reports on where youâ€™ve been, how long you stayed, and how far you traveled.

Works with both old Google Takeout exports and new mobile Timeline exports

Preserves privacy: all processing is local, only coordinates are sent to geocoding APIs

Produces actionable CSVs instead of unreadable JSON

### **What It Does:**
1. **Step 1**: Parses raw Google location JSON files (from mobile devices - NOT the takeout version), allows user to filter by date range, and cleans noisy data by filtering down the data points by duration - do not collect many data points timed close together, by distance - do not collect many data points next to each other, and by probability which is a factor set in the google json.  These filters then are duration in seconds, distance in meters, and probability.  Then the application writes out a parsed json file in the same format as the mobile one, with an added section at the front which defines the dates, and all the filter settings.
2. **Step 2**: Geocodes coordinates from the parsed file to cities/countries and generates travel analysis reports (CSV + HTML views). There are 3 views - days in each city, days in each state (or if outside US by country), and city jumps - this shows travel from city to city with distance travelled between each, and total distance travelled.

Data is stored in a folder structure, but also in a DB via a javascript storage manager.  The files in this system are Master - the original JSON from google, and the parsed files which are the result of the app parsing the Master file.  Clearing the browser cache will destroy any of these files.  It would be nice to be able to RELOAD these into the storage manager if the browser is reset, OR allow the user to optionally load them into the storage manager from the local files.

### User Account Setup
In order to allow different people to use this app, there is a simple user model, which is designed to allow/require a user to use their own API keys, and to work only on their data.  This system creates a user file inside processed folder with the folder there as username. So mainapp/processed/<username> will  contain any data files they have parsed.  In addition, inside the config folder there is a folder called users which contains folders for each username. This folder contains that user's api keys and geo_cache file.  It is supposed to also contain the web_config.json file that contains each user's geoappify and google maps API keys, but in our current version this is broken.  All files each user uploads (master files usually) should be stored in their uploads folder - eg /uploads/<username>/location-history.json.

Features

ğŸ”‘ Multi-user support â€” isolated workspaces per user/session

ğŸ“‚ Custom parsed JSON format â€” speeds up re-runs, embeds filters, and preserves determinism

âš¡ Caching & batch geocoding â€” minimize API calls

ğŸ—ºï¸ City jump detection â€” captures short visits (e.g., Mercer Island â†’ Seattle â†’ Hailey)

âœˆï¸ Distance & mode inference â€” calculates miles traveled and travel mode (driving, flight, walking, etc.)

ğŸ“Š Rich exports:

location_days_by_month_city.csv (or state/country)

city_jumps_with_mode.csv (chronological jumps with distance + inferred mode)

âš™ï¸ Configurable thresholds â€” distance, time, probability, API batching


(Raw JSON â†’ Parsed JSON â†’ Geocoding â†’ Analysis â†’ Exports)

Quick Start
Requirements

Python 3.9+

Geoapify API key (required)

Google Maps API key (optional)

Install
git clone https://github.com/tgonser/WWI.git
cd WWI
pip install -r requirements.txt

Run
python unified_app.py


Open a browser to:
ğŸ‘‰ http://localhost:5000

Multi-User Sessions

This app supports multiple users by isolating workspaces under ./data/<user_id>/:

data/
 â””â”€â”€ <user_id>/
      â”œâ”€â”€ uploads/     # raw Google JSON exports
      â”œâ”€â”€ parsed/      # normalized parsed JSON
      â”œâ”€â”€ cache/       # reverse-geocoding cache
      â””â”€â”€ exports/     # CSV/HTML outputs

Full file structure-
Folder PATH listing for volume Overflo
Volume serial number is 047D-A18A
C:\USERS\TOM\PROGRAMMING\UAPP3
Â¦   .env.txt
Â¦   .gitignore.txt
Â¦   analyzer_bridge.py
Â¦   csv_exporter.py
Â¦   filestructure.txt
Â¦   geo_utils.py
Â¦   legacy_analyzer.py
Â¦   location_analyzer.py
Â¦   modern_analyzer_bridge.py
Â¦   next bugs to fix 8_30.txt
Â¦   parser_app.py
Â¦   readme.md
Â¦   requirements.txt
Â¦   settings.json
Â¦   testing data density.txt
Â¦   unified_app.py
Â¦   
+---config
Â¦   Â¦   geo_cache.json
Â¦   Â¦   secret.txt
Â¦   Â¦   unified_config.json
Â¦   Â¦   users.json
Â¦   Â¦   web_config.json
Â¦   Â¦   
Â¦   +---users
Â¦       +---gonz
Â¦       Â¦       config.json
Â¦       Â¦       geo_cache.json
Â¦       Â¦       
Â¦       +---tom
Â¦               config.json
Â¦               geo_cache.json
Â¦               
+---outputs
Â¦   +---gonz
Â¦   Â¦   +---analysis_20250903_095651_fefb6d90
Â¦   Â¦           
Â¦   +---tom
Â¦       +---analysis_20250904_080505_76e62d37
Â¦               
+---processed
Â¦   Â¦   
Â¦   +---gonz
Â¦   Â¦       01-02-22__04-01-22_parsed_2000_0.5_2000.json
Â¦   Â¦       
Â¦   +---tom
Â¦           11-01-24__01-04-25_parsed_2000_0.1_600.json
Â¦           
+---static
Â¦   +---css
Â¦   Â¦       storage-styles.css
Â¦   Â¦       
Â¦   +---js
Â¦           storage-integration.js
Â¦           storage-manager.js
Â¦           
+---templates
Â¦       login.html
Â¦       processing.html
Â¦       register.html
Â¦       results.html
Â¦       unified_processor.html
Â¦       
+---uploads
Â¦   +---gonz
Â¦   +---tom
        



ğŸ”— See Multi-User Sessions
 for details.

Custom Parsed JSON Format

After parsing, the app writes a normalized JSON file that:

Saves the applied filters (date range, thresholds, probability)

Records all points chronologically with inferred metadata

Can be reused to skip raw parsing on future runs

ğŸ”— See Custom Parsed JSON Format
.

Configuration

API keys:

GEOAPIFY_API_KEY (required)

GOOGLE_MAPS_API_KEY (optional)

Settings:
config/settings.json controls thresholds and defaults:

Distance threshold: ~600â€“2000 m

Time threshold: ~100â€“500 s

Probability threshold: ~0.25 (raising above ~0.65â€“0.8 may skip visits)

.env example:

GEOAPIFY_API_KEY=your_key_here
GOOGLE_MAPS_API_KEY=optional_google_key

Privacy

All processing is local

Only coordinates are sent to geocoding APIs

Results and caches are stored under your own ./data/<user_id>/

You control API usage, batch sizes, and retention

Contributing

Contributions welcome!
Please open issues or PRs for bugs, improvements, or new features.

License

MIT License (recommended â€” add LICENSE file at repo root).

Documents

Multi-User Sessions

Custom Parsed JSON Format

âœ¨ With this README, newcomers will immediately see:

What the app does

Why itâ€™s useful

How to run it

Where to learn the advanced details